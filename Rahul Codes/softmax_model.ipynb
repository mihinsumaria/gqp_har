{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'softmax_helper' from '/Users/rahul/python_projects/gqp-takeda/notebooks/softmax_helper.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "import softmax_helper\n",
    "from importlib import reload\n",
    "reload(softmax_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "label_info = pd.read_csv('../data/phase1/activity_labels.txt', delim_whitespace=True, header=None, index_col=False)\n",
    "label_mapping = dict()\n",
    "\n",
    "for index, row_data in label_info.iterrows():\n",
    "    label_mapping.update({row_data[0]-1 : row_data[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('../data/phase1/train/X_train.npy')\n",
    "Y_train = np.load('../data/phase1/train/Y_train.npy')\n",
    "X_valid = np.load('../data/phase1/train/X_valid.npy')\n",
    "Y_valid = np.load('../data/phase1/train/Y_valid.npy')\n",
    "X_test  = np.load('../data/phase1/train/X_test.npy')\n",
    "Y_test  = np.load('../data/phase1/train/Y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6213, 86), (6213, 12), (1554, 86), (1554, 12), (3162, 561), (3162, 12))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape, X_valid.shape, Y_valid.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4; mini_batch_size = 128; learning_rate = 3e-2; epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss: 1.0295878303478663; epoch: 0; accuracy: 0.6933333333333334\n",
      "Current loss: 0.3649937484959556; epoch: 20; accuracy: 0.8066666666666666\n",
      "Current loss: 0.24511965262405522; epoch: 40; accuracy: 0.88\n",
      "Current loss: 0.2370731520933753; epoch: 60; accuracy: 0.8733333333333333\n",
      "Current loss: 0.2508971059585292; epoch: 80; accuracy: 0.8466666666666667\n",
      "Current loss: 0.22914589533983382; epoch: 100; accuracy: 0.8533333333333334\n",
      "Current loss: 0.18669154477366964; epoch: 120; accuracy: 0.8733333333333333\n",
      "Current loss: 0.19631089816919672; epoch: 140; accuracy: 0.8733333333333333\n",
      "Current loss: 0.1704051044922564; epoch: 160; accuracy: 0.9133333333333333\n",
      "Current loss: 0.1574273379501263; epoch: 180; accuracy: 0.9\n",
      "Current loss: 0.1482948844748124; epoch: 200; accuracy: 0.9133333333333333\n",
      "Current loss: 0.14066152895583106; epoch: 220; accuracy: 0.9266666666666666\n",
      "Current loss: 0.201851698391325; epoch: 240; accuracy: 0.84\n",
      "Current loss: 0.15468262674311067; epoch: 260; accuracy: 0.8933333333333333\n",
      "Current loss: 0.12695639998674313; epoch: 280; accuracy: 0.94\n",
      "Current loss: 0.1455100873662129; epoch: 300; accuracy: 0.9\n",
      "Current loss: 0.1548360059559914; epoch: 320; accuracy: 0.8866666666666667\n",
      "Current loss: 0.13546631846868784; epoch: 340; accuracy: 0.9133333333333333\n",
      "Current loss: 0.12404590521839477; epoch: 360; accuracy: 0.9333333333333333\n",
      "Current loss: 0.1523052180939267; epoch: 380; accuracy: 0.9066666666666666\n",
      "Current loss: 0.12245543092122504; epoch: 400; accuracy: 0.92\n",
      "Current loss: 0.12299086544394096; epoch: 420; accuracy: 0.9466666666666667\n",
      "Current loss: 0.13539295683660493; epoch: 440; accuracy: 0.92\n",
      "Current loss: 0.12267414414108033; epoch: 460; accuracy: 0.9466666666666667\n",
      "Current loss: 0.12424539040512832; epoch: 480; accuracy: 0.9333333333333333\n",
      "Current loss: 0.14139434298405532; epoch: 500; accuracy: 0.9066666666666666\n",
      "Current loss: 0.13884799304541223; epoch: 520; accuracy: 0.9\n",
      "Current loss: 0.10413110631412227; epoch: 540; accuracy: 0.9466666666666667\n",
      "Current loss: 0.1559689845791818; epoch: 560; accuracy: 0.9066666666666666\n",
      "Current loss: 0.10186421075336338; epoch: 580; accuracy: 0.9466666666666667\n",
      "Current loss: 0.09909212566006363; epoch: 600; accuracy: 0.9533333333333334\n",
      "Current loss: 0.09342946345193713; epoch: 620; accuracy: 0.96\n",
      "Current loss: 0.08426976524596289; epoch: 640; accuracy: 0.9666666666666667\n",
      "Current loss: 0.10222877809210325; epoch: 660; accuracy: 0.9333333333333333\n",
      "Current loss: 0.13368772443343746; epoch: 680; accuracy: 0.9266666666666666\n",
      "Current loss: 0.12463004744219372; epoch: 700; accuracy: 0.9066666666666666\n",
      "Current loss: 0.09645255234873401; epoch: 720; accuracy: 0.9733333333333334\n",
      "Current loss: 0.11730548100536539; epoch: 740; accuracy: 0.9133333333333333\n",
      "Current loss: 0.15497398297117812; epoch: 760; accuracy: 0.8933333333333333\n",
      "Current loss: 0.12400058971616953; epoch: 780; accuracy: 0.9466666666666667\n",
      "Current loss: 0.1550689302501468; epoch: 800; accuracy: 0.8866666666666667\n",
      "Current loss: 0.1816828161937551; epoch: 820; accuracy: 0.9\n",
      "Current loss: 0.11911986611209116; epoch: 840; accuracy: 0.92\n",
      "Current loss: 0.10055019864632991; epoch: 860; accuracy: 0.9333333333333333\n",
      "Current loss: 0.09263956411501566; epoch: 880; accuracy: 0.9533333333333334\n",
      "Current loss: 0.11101664044224015; epoch: 900; accuracy: 0.9333333333333333\n",
      "Current loss: 0.08676506867548019; epoch: 920; accuracy: 0.9533333333333334\n",
      "Current loss: 0.1189616858792145; epoch: 940; accuracy: 0.92\n",
      "Current loss: 0.14608918819053268; epoch: 960; accuracy: 0.9066666666666666\n",
      "Current loss: 0.09940422281607031; epoch: 980; accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "w = softmax_helper.stochastic_gradient_descent(X_train, Y_train, alpha, mini_batch_size, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_w = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss: 0.11293542827395964\n",
      "Final training accuracy: 0.9332045710606792\n"
     ]
    }
   ],
   "source": [
    "train_probs = softmax_helper.get_probs(X_train, best_w)\n",
    "train_loss = softmax_helper.cross_entropy_loss(Y_train, train_probs)\n",
    "train_accuracy = softmax_helper.get_accuracy(Y_train, train_probs)\n",
    "print(\"Final training loss: {}\".format(train_loss))\n",
    "print(\"Final training accuracy: {}\".format(train_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.13046773752750365\n",
      "Validation accuracy: 0.9111969111969112\n"
     ]
    }
   ],
   "source": [
    "valid_probs = softmax_helper.get_probs(X_valid, best_w)\n",
    "valid_loss = softmax_helper.cross_entropy_loss(Y_valid, valid_probs)\n",
    "valid_accuracy = softmax_helper.get_accuracy(Y_valid, valid_probs)\n",
    "print(\"Validation loss: {}\".format(valid_loss))\n",
    "print(\"Validation accuracy: {}\".format(valid_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           WALKING       0.97      0.97      0.97       971\n",
      "  WALKING_UPSTAIRS       0.97      0.98      0.97       869\n",
      "WALKING_DOWNSTAIRS       0.96      0.98      0.97       783\n",
      "           SITTING       0.88      0.86      0.87      1030\n",
      "          STANDING       0.90      0.91      0.90      1143\n",
      "            LAYING       0.95      0.94      0.94      1129\n",
      "      STAND_TO_SIT       0.91      0.79      0.85        38\n",
      "      SIT_TO_STAND       0.89      0.89      0.89        19\n",
      "        SIT_TO_LIE       0.86      0.86      0.86        57\n",
      "        LIE_TO_SIT       0.92      0.85      0.88        53\n",
      "      STAND_TO_LIE       0.91      0.85      0.88        73\n",
      "      LIE_TO_STAND       0.85      0.85      0.85        48\n",
      "\n",
      "         micro avg       0.93      0.93      0.93      6213\n",
      "         macro avg       0.91      0.90      0.90      6213\n",
      "      weighted avg       0.93      0.93      0.93      6213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        y_true=Y_train.argmax(axis=1),\n",
    "        y_pred=train_probs.argmax(axis=1),\n",
    "        labels=list(label_mapping.keys()),\n",
    "        target_names=list(label_mapping.values()),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           WALKING       0.97      0.95      0.96       255\n",
      "  WALKING_UPSTAIRS       0.93      0.97      0.95       204\n",
      "WALKING_DOWNSTAIRS       0.96      0.97      0.96       204\n",
      "           SITTING       0.84      0.84      0.84       263\n",
      "          STANDING       0.87      0.88      0.87       280\n",
      "            LAYING       0.95      0.93      0.94       284\n",
      "      STAND_TO_SIT       0.75      0.67      0.71         9\n",
      "      SIT_TO_STAND       1.00      0.75      0.86         4\n",
      "        SIT_TO_LIE       1.00      0.61      0.76        18\n",
      "        LIE_TO_SIT       0.71      0.71      0.71         7\n",
      "      STAND_TO_LIE       0.72      0.76      0.74        17\n",
      "      LIE_TO_STAND       0.62      0.56      0.59         9\n",
      "\n",
      "         micro avg       0.91      0.91      0.91      1554\n",
      "         macro avg       0.86      0.80      0.82      1554\n",
      "      weighted avg       0.91      0.91      0.91      1554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        y_true=Y_valid.argmax(axis=1),\n",
    "        y_pred=valid_probs.argmax(axis=1),\n",
    "        labels=list(label_mapping.keys()),\n",
    "        target_names=list(label_mapping.values())\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparam grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_epochs = [10, 50, 100, 200]\n",
    "all_batch_sizes = [100, 200, 500, 1000]\n",
    "all_learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
    "all_alphas = [0.0001, 0.01, 0.1]\n",
    "\n",
    "min_loss = np.inf\n",
    "best_params = (0,0,0,0)\n",
    "\n",
    "all_combinations = list(product(all_epochs, all_batch_sizes, all_alphas, all_learning_rates,))\n",
    "\n",
    "print(\"# hyper param comb: {}\".format(len(all_combinations)))\n",
    "input(\"Press Enter to continue...\")\n",
    "\n",
    "for param in all_combinations:\n",
    "    try:\n",
    "        epochs, mini_batch_size, alpha, learning_rate = param\n",
    "\n",
    "        print(\"epochs: {}; mini_batch_size: {}; alpha: {}; learning_rate: {}\".format(*param))\n",
    "\n",
    "        w = softmax_helper.stochastic_gradient_descent(X_train, Y_train, alpha, mini_batch_size, learning_rate, epochs)\n",
    "\n",
    "        valid_probs = softmax_helper.get_probs(X_valid, w)\n",
    "        valid_loss = softmax_helper.cross_entropy_loss(Y_valid, valid_probs)\n",
    "        valid_accuracy = softmax_helper.get_accuracy(Y_valid, valid_probs)\n",
    "        print(\"Valid loss: {}\".format(valid_loss))\n",
    "        print(\"Valid accuracy: {}\".format(valid_accuracy))\n",
    "\n",
    "        if valid_loss < min_loss:\n",
    "            min_loss = valid_loss\n",
    "            best_params = param\n",
    "    except KeyboardInterrupt as e:\n",
    "        break\n",
    "\n",
    "epochs, mini_batch_size, alpha, learning_rate = best_params\n",
    "finalTrainData = np.append(X_train, X_valid, axis=0)\n",
    "finalTrainLabels = np.append(Y_train, Y_valid, axis=0)\n",
    "\n",
    "best_w = softmax_helper.stochastic_gradient_descent(finalTrainData, finalTrainLabels, alpha, mini_batch_size, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_probs = softmax_helper.get_probs(X_train, best_w)\n",
    "train_loss = softmax_helper.cross_entropy_loss(Y_train, train_probs)\n",
    "train_accuracy = softmax_helper.get_accuracy(Y_train, train_probs)\n",
    "print(\"Final training loss: {}\".format(train_loss))\n",
    "print(\"Final training accuracy: {}\".format(train_accuracy))\n",
    "\n",
    "valid_probs = softmax_helper.get_probs(X_valid, best_w)\n",
    "valid_loss = softmax_helper.cross_entropy_loss(Y_valid, valid_probs)\n",
    "valid_accuracy = softmax_helper.get_accuracy(Y_valid, valid_probs)\n",
    "print(\"Validation loss: {}\".format(valid_loss))\n",
    "print(\"Validation accuracy: {}\".format(valid_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
